{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T13:34:07.694866Z",
     "start_time": "2024-03-23T13:34:07.159686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<duckdb.duckdb.DuckDBPyConnection at 0x107dcaff0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# some DuckDB setup\n",
    "con = duckdb.connect(database=':memory:')\n",
    "# enable automatic query parallelization\n",
    "con.execute(\"PRAGMA threads=2\")\n",
    "# enable caching of parquet metadata\n",
    "con.execute(\"PRAGMA enable_object_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = \"'../data/train-[1-8].parquet'\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:34:07.697824Z",
     "start_time": "2024-03-23T13:34:07.694139Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         tconst                                   primaryTitle  \\\n0     tt0010600                                       The Doll   \n1     tt0011841                                  Way Down East   \n2     tt0012494                                        Destiny   \n3     tt0015163                                  The Navigator   \n4     tt0016220                       The Phantom of the Opera   \n...         ...                                            ...   \n7954  tt9625664                                  Trauma Center   \n7955  tt9741310                                          Slaxx   \n7956  tt9742392                                        Kindred   \n7957  tt9850386  The Bee Gees: How Can You Mend a Broken Heart   \n7958  tt9911196                            The Marriage Escape   \n\n                       originalTitle startYear endYear runtimeMinutes  \\\n0                          Die Puppe      1919      \\N             66   \n1                      Way Down East      1920      \\N            145   \n2                       Der müde Tod      1921      \\N             97   \n3                      The Navigator      1924      \\N             59   \n4           The Phantom of the Opera      1925      \\N             93   \n...                              ...       ...     ...            ...   \n7954                            None      2019      \\N             87   \n7955                           Slaxx      2020      \\N             77   \n7956                         Kindred      2020      \\N            101   \n7957                            None      2020      \\N            111   \n7958  De beentjes van Sint-Hildegard      2020      \\N            103   \n\n      numVotes  label  __index_level_0__  \n0       1898.0   True                  4  \n1       5376.0   True                  7  \n2       5842.0   True                  9  \n3       9652.0   True                 25  \n4      17887.0   True                 38  \n...        ...    ...                ...  \n7954   12951.0  False               9966  \n7955    2464.0  False               9981  \n7956    1719.0  False               9982  \n7957    4144.0   True               9996  \n7958    3242.0   True               9999  \n\n[7959 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tconst</th>\n      <th>primaryTitle</th>\n      <th>originalTitle</th>\n      <th>startYear</th>\n      <th>endYear</th>\n      <th>runtimeMinutes</th>\n      <th>numVotes</th>\n      <th>label</th>\n      <th>__index_level_0__</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt0010600</td>\n      <td>The Doll</td>\n      <td>Die Puppe</td>\n      <td>1919</td>\n      <td>\\N</td>\n      <td>66</td>\n      <td>1898.0</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt0011841</td>\n      <td>Way Down East</td>\n      <td>Way Down East</td>\n      <td>1920</td>\n      <td>\\N</td>\n      <td>145</td>\n      <td>5376.0</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt0012494</td>\n      <td>Destiny</td>\n      <td>Der müde Tod</td>\n      <td>1921</td>\n      <td>\\N</td>\n      <td>97</td>\n      <td>5842.0</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt0015163</td>\n      <td>The Navigator</td>\n      <td>The Navigator</td>\n      <td>1924</td>\n      <td>\\N</td>\n      <td>59</td>\n      <td>9652.0</td>\n      <td>True</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt0016220</td>\n      <td>The Phantom of the Opera</td>\n      <td>The Phantom of the Opera</td>\n      <td>1925</td>\n      <td>\\N</td>\n      <td>93</td>\n      <td>17887.0</td>\n      <td>True</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7954</th>\n      <td>tt9625664</td>\n      <td>Trauma Center</td>\n      <td>None</td>\n      <td>2019</td>\n      <td>\\N</td>\n      <td>87</td>\n      <td>12951.0</td>\n      <td>False</td>\n      <td>9966</td>\n    </tr>\n    <tr>\n      <th>7955</th>\n      <td>tt9741310</td>\n      <td>Slaxx</td>\n      <td>Slaxx</td>\n      <td>2020</td>\n      <td>\\N</td>\n      <td>77</td>\n      <td>2464.0</td>\n      <td>False</td>\n      <td>9981</td>\n    </tr>\n    <tr>\n      <th>7956</th>\n      <td>tt9742392</td>\n      <td>Kindred</td>\n      <td>Kindred</td>\n      <td>2020</td>\n      <td>\\N</td>\n      <td>101</td>\n      <td>1719.0</td>\n      <td>False</td>\n      <td>9982</td>\n    </tr>\n    <tr>\n      <th>7957</th>\n      <td>tt9850386</td>\n      <td>The Bee Gees: How Can You Mend a Broken Heart</td>\n      <td>None</td>\n      <td>2020</td>\n      <td>\\N</td>\n      <td>111</td>\n      <td>4144.0</td>\n      <td>True</td>\n      <td>9996</td>\n    </tr>\n    <tr>\n      <th>7958</th>\n      <td>tt9911196</td>\n      <td>The Marriage Escape</td>\n      <td>De beentjes van Sint-Hildegard</td>\n      <td>2020</td>\n      <td>\\N</td>\n      <td>103</td>\n      <td>3242.0</td>\n      <td>True</td>\n      <td>9999</td>\n    </tr>\n  </tbody>\n</table>\n<p>7959 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(f\"\"\"\n",
    "\n",
    "    select *\n",
    "    from {train_data}\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:34:10.179419Z",
     "start_time": "2024-03-23T13:34:07.697601Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Create train and test split from the beginning for comparability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "film_ids = con.execute(f\"\"\"\n",
    "    select distinct tconst,\n",
    "        label\n",
    "    from {train_data}\n",
    "    \"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:34:10.179600Z",
     "start_time": "2024-03-23T13:34:10.168023Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training data in train and test split\n",
    "\n",
    "ids = film_ids.drop('label', axis=1).values\n",
    "labels = film_ids.loc[:, 'label'].values\n",
    "\n",
    "# Small test size because cross-validation (and want to train on more data)\n",
    "X_train_ids, X_test_ids, y_train, y_test = train_test_split(ids, labels, test_size=0.10, random_state=42, stratify=labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:34:12.110702Z",
     "start_time": "2024-03-23T13:34:10.172750Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train_ids  = pd.DataFrame(X_train_ids, columns=['tconst'])\n",
    "X_test_ids = pd.DataFrame(X_test_ids, columns=['label'])\n",
    "\n",
    "X_train_ids.to_parquet(\"X_train_ids.parquet\")\n",
    "X_test_ids.to_parquet(\"X_test_ids.parquet\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:37:50.290884Z",
     "start_time": "2024-03-23T13:37:50.275670Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ids_train = \"'X_train_ids.parquet'\"\n",
    "ids_test = \"'X_test_ids.parquet'\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:37:51.610320Z",
     "start_time": "2024-03-23T13:37:51.593626Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Establish Baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:36:15.943918Z",
     "start_time": "2024-03-23T13:36:15.655322Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset as it is\n",
    "\n",
    "Dropping NaN in train set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "baseline_data_train = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    " select tconst, \n",
    "        try_cast(startYear as integer) as year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_train})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:34.649821Z",
     "start_time": "2024-03-23T13:39:34.637458Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "baseline_data_test = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    "    select tconst, \n",
    "        try_cast(startYear as integer) as year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_test})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:35.361501Z",
     "start_time": "2024-03-23T13:39:34.865048Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "baseline_data_train_noNAN = baseline_data_train.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:35.618312Z",
     "start_time": "2024-03-23T13:39:35.602286Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = baseline_data_train_noNAN.drop(['tconst',  'label'], axis=1).values\n",
    "y_train = baseline_data_train_noNAN.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:35.860738Z",
     "start_time": "2024-03-23T13:39:35.846573Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5812, 5)\n",
      "(5812,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:36.121663Z",
     "start_time": "2024-03-23T13:39:36.110172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = baseline_data_test.drop(['tconst',  'label'], axis=1).values\n",
    "y_test = baseline_data_test.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:36.349716Z",
     "start_time": "2024-03-23T13:39:36.334157Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7959, 5)\n",
      "(7959,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:39:57.588558Z",
     "start_time": "2024-03-23T13:39:57.548578Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "XGBoost best parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200, 'classifier__subsample': 0.8}\n",
      "XGBoost best score: 0.7355\n",
      "Accuracy: 74.13% 73.99%\n",
      "Confusion Matrix:\n",
      "[[3095  874]\n",
      " [1185 2805]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.72      0.78      0.75      3969\n",
      "        True       0.76      0.70      0.73      3990\n",
      "\n",
      "    accuracy                           0.74      7959\n",
      "   macro avg       0.74      0.74      0.74      7959\n",
      "weighted avg       0.74      0.74      0.74      7959\n",
      "\n",
      "0.5048176187198898 0.5013192612137203 0.4622440005025757 0.45897174\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Base model pipelines\n",
    "models = {\n",
    "    'XGBoost': Pipeline(steps=[('preprocessing', preprocessing), ('classifier', xgb.XGBClassifier())])\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 150, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [2, 3, 4, 5],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for model_name, model_pipeline in models.items():\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(model_pipeline, hyperparameters[model_name], cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"{model_name} best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} best score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Use the best estimator to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Test if adjusting the threshold helps improve the accuracy (knowing dataset is balanced)\n",
    "    y_proba = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    threshold = np.median(y_proba)\n",
    "    y_pred_th = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% {accuracy_score(y_test, y_pred_th) * 100:.2f}%\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # See how balanced the predictions are\n",
    "    print (y_train.mean(), y_test.mean(), y_pred.mean(), threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:40:04.047766Z",
     "start_time": "2024-03-23T13:39:59.395740Z"
    }
   },
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keeping NaN and replacing and imputing them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = baseline_data_train.drop(['tconst',  'label'], axis=1).values\n",
    "y_train = baseline_data_train.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:43:55.393034Z",
     "start_time": "2024-03-23T13:43:55.372592Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = baseline_data_test.drop(['tconst',  'label'], axis=1).values\n",
    "y_test = baseline_data_test.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:43:55.994817Z",
     "start_time": "2024-03-23T13:43:55.976326Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "XGBoost best parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "XGBoost best score: 0.7290\n",
      "Accuracy: 75.96% 75.45%\n",
      "Confusion Matrix:\n",
      "[[3190  779]\n",
      " [1134 2856]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.80      0.77      3969\n",
      "        True       0.79      0.72      0.75      3990\n",
      "\n",
      "    accuracy                           0.76      7959\n",
      "   macro avg       0.76      0.76      0.76      7959\n",
      "weighted avg       0.76      0.76      0.76      7959\n",
      "\n",
      "0.5013262599469496 0.5013192612137203 0.456715667797462 0.44629627\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Base model pipelines\n",
    "models = {\n",
    "    'XGBoost': Pipeline(steps=[('preprocessing', preprocessing), ('classifier', xgb.XGBClassifier())])\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 150, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [2, 3, 4, 5],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for model_name, model_pipeline in models.items():\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(model_pipeline, hyperparameters[model_name], cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"{model_name} best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} best score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Use the best estimator to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Test if adjusting the threshold helps improve the accuracy (knowing dataset is balanced)\n",
    "    y_proba = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    threshold = np.median(y_proba)\n",
    "    y_pred_th = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% {accuracy_score(y_test, y_pred_th) * 100:.2f}%\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # See how balanced the predictions are\n",
    "    print (y_train.mean(), y_test.mean(), y_pred.mean(), threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:44:47.453273Z",
     "start_time": "2024-03-23T13:44:43.182700Z"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixing the year issue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "train_fixed_year = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    " select tconst, \n",
    "        -- Merge start year and end year into single column\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_train})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:44:52.713902Z",
     "start_time": "2024-03-23T13:44:52.691211Z"
    }
   },
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "test_fixed_year = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    "    select tconst, \n",
    "        -- Merge start year and end year into single column\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_test})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:44:53.449990Z",
     "start_time": "2024-03-23T13:44:52.917922Z"
    }
   },
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = train_fixed_year.drop(['tconst',  'label'], axis=1).values\n",
    "y_train = train_fixed_year.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:44:53.964070Z",
     "start_time": "2024-03-23T13:44:53.954319Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = test_fixed_year.drop(['tconst',  'label'], axis=1).values\n",
    "y_test = test_fixed_year.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:44:54.257246Z",
     "start_time": "2024-03-23T13:44:54.246477Z"
    }
   },
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "XGBoost best parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "XGBoost best score: 0.7290\n",
      "Accuracy: 75.96% 75.45%\n",
      "Confusion Matrix:\n",
      "[[3190  779]\n",
      " [1134 2856]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.80      0.77      3969\n",
      "        True       0.79      0.72      0.75      3990\n",
      "\n",
      "    accuracy                           0.76      7959\n",
      "   macro avg       0.76      0.76      0.76      7959\n",
      "weighted avg       0.76      0.76      0.76      7959\n",
      "\n",
      "0.5013262599469496 0.5013192612137203 0.456715667797462 0.44629627\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Base model pipelines\n",
    "models = {\n",
    "    'XGBoost': Pipeline(steps=[('preprocessing', preprocessing), ('classifier', xgb.XGBClassifier())])\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 150, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [2, 3, 4, 5],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for model_name, model_pipeline in models.items():\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(model_pipeline, hyperparameters[model_name], cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"{model_name} best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} best score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Use the best estimator to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Test if adjusting the threshold helps improve the accuracy (knowing dataset is balanced)\n",
    "    y_proba = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    threshold = np.median(y_proba)\n",
    "    y_pred_th = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% {accuracy_score(y_test, y_pred_th) * 100:.2f}%\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # See how balanced the predictions are\n",
    "    print (y_train.mean(), y_test.mean(), y_pred.mean(), threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:02.460495Z",
     "start_time": "2024-03-23T13:44:57.966019Z"
    }
   },
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding additional features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine all films listed in the writer/director files with the year of the film \n",
    "# (from training, validation and test data).\n",
    "movie_year = con.execute('''\n",
    "    SELECT tconst,\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS Year,\n",
    "    FROM '../data/train-[1-8].parquet'\n",
    "    UNION\n",
    "    SELECT tconst,\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS Year,\n",
    "    FROM '../data/test_hidden.parquet'\n",
    "    UNION\n",
    "    SELECT tconst,\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS Year,\n",
    "    FROM '../data/validation_hidden.parquet'\n",
    "    ORDER BY Year, tconst\n",
    "''').df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:21.920556Z",
     "start_time": "2024-03-23T13:45:21.901487Z"
    }
   },
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate the number of films worked on previously by the writers and directors\n",
    "# Includes the current film\n",
    "\n",
    "experience = con.execute('''\n",
    "    with writers as (\n",
    "        select *\n",
    "        from '../data/writing.parquet'\n",
    "    ), \n",
    "    directors as (\n",
    "        select * \n",
    "        from '../data/directing.parquet'\n",
    "    \n",
    "    )\n",
    "    SELECT tconst, Year, writer, director,\n",
    "    COUNT(DISTINCT tconst) OVER(PARTITION BY writer ORDER BY Year, tconst) AS writer_experience,\n",
    "    COUNT(DISTINCT tconst) OVER(PARTITION BY director ORDER BY Year, tconst) AS director_experience,\n",
    "    FROM movie_year my\n",
    "    LEFT JOIN writers ON writers.movie == my.tconst\n",
    "    LEFT JOIN directors ON directors.movie == my.tconst\n",
    "    ORDER BY Year, tconst\n",
    "''').df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:22.516190Z",
     "start_time": "2024-03-23T13:45:22.473902Z"
    }
   },
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "train_features = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    " select tconst, \n",
    "        -- Merge start year and end year into single column\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_train})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:22.968602Z",
     "start_time": "2024-03-23T13:45:22.955636Z"
    }
   },
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "test_features = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    "    select tconst, \n",
    "        -- Merge start year and end year into single column\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_test})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:23.936276Z",
     "start_time": "2024-03-23T13:45:23.498790Z"
    }
   },
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge movie data with the experience level of the writers and directors.\n",
    "# For each film, calculate the number of writers/directors, average experience and total experience\n",
    "\n",
    "train_all_features = con.execute('''\n",
    "    SELECT \n",
    "      ANY_VALUE(td.tconst) AS tconst, \n",
    "      ANY_VALUE(td.Year) AS Year, \n",
    "      ANY_VALUE(runtime) AS runtimeMinutes,\n",
    "      ANY_VALUE(ForeignFilm) AS ForeignFilm, \n",
    "      ANY_VALUE(n_words) AS n_words, \n",
    "      ANY_VALUE(votes) AS numVotes,\n",
    "      COUNT(DISTINCT writer) AS n_writers,\n",
    "      AVG(DISTINCT e.writer_experience) AS avgexp_writers,\n",
    "      SUM(DISTINCT e.writer_experience) AS totexp_writers,\n",
    "      COUNT(DISTINCT director) AS n_directors,\n",
    "      AVG(DISTINCT e.director_experience) AS avgexp_directors,\n",
    "      SUM(DISTINCT e.director_experience) AS totexp_directors,\n",
    "      ANY_VALUE(label) AS label\n",
    "    FROM train_features td\n",
    "    LEFT JOIN experience e ON e.tconst == td.tconst\n",
    "    GROUP BY td.tconst\n",
    "''').df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:24.274723Z",
     "start_time": "2024-03-23T13:45:24.243887Z"
    }
   },
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge movie data with the experience level of the writers and directors.\n",
    "# For each film, calculate the number of writers/directors, average experience and total experience\n",
    "\n",
    "test_all_features = con.execute('''\n",
    "    SELECT \n",
    "      ANY_VALUE(td.tconst) AS tconst, \n",
    "      ANY_VALUE(td.Year) AS Year, \n",
    "      ANY_VALUE(runtime) AS runtimeMinutes,\n",
    "      ANY_VALUE(ForeignFilm) AS ForeignFilm, \n",
    "      ANY_VALUE(n_words) AS n_words, \n",
    "      ANY_VALUE(votes) AS numVotes,\n",
    "      COUNT(DISTINCT writer) AS n_writers,\n",
    "      AVG(DISTINCT e.writer_experience) AS avgexp_writers,\n",
    "      SUM(DISTINCT e.writer_experience) AS totexp_writers,\n",
    "      COUNT(DISTINCT director) AS n_directors,\n",
    "      AVG(DISTINCT e.director_experience) AS avgexp_directors,\n",
    "      SUM(DISTINCT e.director_experience) AS totexp_directors,\n",
    "      ANY_VALUE(label) AS label\n",
    "    FROM test_features td\n",
    "    LEFT JOIN experience e ON e.tconst == td.tconst\n",
    "    GROUP BY td.tconst\n",
    "''').df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:24.898189Z",
     "start_time": "2024-03-23T13:45:24.873936Z"
    }
   },
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = train_all_features.drop(['tconst',  'label'], axis=1).values\n",
    "y_train = train_all_features.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:26.989130Z",
     "start_time": "2024-03-23T13:45:26.973843Z"
    }
   },
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = test_all_features.drop(['tconst',  'label'], axis=1).values\n",
    "y_test = test_all_features.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:27.288633Z",
     "start_time": "2024-03-23T13:45:27.271200Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "XGBoost best parameters: {'classifier__learning_rate': 0.05, 'classifier__max_depth': 5, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "XGBoost best score: 0.7038\n",
      "Accuracy: 76.77% 76.98%\n",
      "Confusion Matrix:\n",
      "[[3242  727]\n",
      " [1122 2868]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.82      0.78      3969\n",
      "        True       0.80      0.72      0.76      3990\n",
      "\n",
      "    accuracy                           0.77      7959\n",
      "   macro avg       0.77      0.77      0.77      7959\n",
      "weighted avg       0.77      0.77      0.77      7959\n",
      "\n",
      "0.5013262599469496 0.5013192612137203 0.4516899107928132 0.4436876\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Base model pipelines\n",
    "models = {\n",
    "    'XGBoost': Pipeline(steps=[('preprocessing', preprocessing), ('classifier', xgb.XGBClassifier())])\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 150, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [2, 3, 4, 5],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for model_name, model_pipeline in models.items():\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(model_pipeline, hyperparameters[model_name], cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"{model_name} best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} best score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Use the best estimator to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Test if adjusting the threshold helps improve the accuracy (knowing dataset is balanced)\n",
    "    y_proba = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    threshold = np.median(y_proba)\n",
    "    y_pred_th = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% {accuracy_score(y_test, y_pred_th) * 100:.2f}%\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # See how balanced the predictions are\n",
    "    print (y_train.mean(), y_test.mean(), y_pred.mean(), threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:37.263249Z",
     "start_time": "2024-03-23T13:45:31.672985Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "**All features**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "train_features = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    " select tconst, \n",
    "    -- Clean up the movie title text. Remove excess whitespace, convert to lowercase, convert non-ascii to ascii equivalent, \n",
    "        -- remove everything that is non-alpanumeric or a space.\n",
    "        REGEXP_REPLACE(TRANSLATE(LOWER(TRIM(primaryTitle)), 'áàãäåæßçéèêíîïñòóôöøớúûüý','aaaaaabceeeiiinoooooouuuy'),'[^a-zA-Z0-9 ]','','g') AS pTitle,\n",
    "        REGEXP_REPLACE(TRANSLATE(LOWER(TRIM(originalTitle)),'áàãäåæßçéèêíîïñòóôöøớúûüý','aaaaaabceeeiiinoooooouuuy'),'[^a-zA-Z0-9 ]','','g') AS oTitle,\n",
    "        \n",
    "        -- Merge start year and end year into single column\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_train})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:41.311195Z",
     "start_time": "2024-03-23T13:45:41.276503Z"
    }
   },
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load all the data into the duck database\n",
    "test_features = con.execute(f\"\"\"\n",
    "    with train as (\n",
    "        select * \n",
    "        from {train_data}\n",
    "    )\n",
    "    select tconst, \n",
    "       -- Clean up the movie title text. Remove excess whitespace, convert to lowercase, convert non-ascii to ascii equivalent, \n",
    "        -- remove everything that is non-alpanumeric or a space.\n",
    "        REGEXP_REPLACE(TRANSLATE(LOWER(TRIM(primaryTitle)), 'áàãäåæßçéèêíîïñòóôöøớúûüý','aaaaaabceeeiiinoooooouuuy'),'[^a-zA-Z0-9 ]','','g') AS pTitle,\n",
    "        REGEXP_REPLACE(TRANSLATE(LOWER(TRIM(originalTitle)),'áàãäåæßçéèêíîïñòóôöøớúûüý','aaaaaabceeeiiinoooooouuuy'),'[^a-zA-Z0-9 ]','','g') AS oTitle,\n",
    "        \n",
    "        -- Merge start year and end year into single column\n",
    "        CASE\n",
    "            WHEN startYear LIKE '%N%' THEN endYear\n",
    "            ELSE startYear\n",
    "        END AS year,\n",
    "        try_cast(runtimeMinutes as integer) as runtime, \n",
    "        try_cast(numVotes as integer) as votes,\n",
    "        CASE\n",
    "           WHEN originalTitle IS NULL THEN 0\n",
    "           ELSE 1\n",
    "        END AS ForeignFilm,\n",
    "        -- Count number of words in title\n",
    "        LENGTH(primaryTitle) - LENGTH(REPLACE(primaryTitle, ' ', '')) + 1 AS n_words,\n",
    "        label\n",
    "    from train\n",
    "    where train.tconst in (select tconst from {ids_test})\n",
    "\n",
    "\"\"\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:42.173329Z",
     "start_time": "2024-03-23T13:45:41.608366Z"
    }
   },
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge movie data with the experience level of the writers and directors.\n",
    "# For each film, calculate the number of writers/directors, average experience and total experience\n",
    "\n",
    "train_fixed_title = con.execute('''\n",
    "    SELECT \n",
    "    \n",
    "      ANY_VALUE(td.tconst) AS tconst, \n",
    "      ANY_VALUE(td.Year) AS Year, \n",
    "      ANY_VALUE(runtime) AS runtimeMinutes,\n",
    "      ANY_VALUE(ForeignFilm) AS ForeignFilm, \n",
    "      ANY_VALUE(n_words) AS n_words, \n",
    "      ANY_VALUE(votes) AS numVotes,\n",
    "      COUNT(DISTINCT writer) AS n_writers,\n",
    "      AVG(DISTINCT e.writer_experience) AS avgexp_writers,\n",
    "      SUM(DISTINCT e.writer_experience) AS totexp_writers,\n",
    "      COUNT(DISTINCT director) AS n_directors,\n",
    "      AVG(DISTINCT e.director_experience) AS avgexp_directors,\n",
    "      SUM(DISTINCT e.director_experience) AS totexp_directors,\n",
    "      ANY_VALUE(label) AS label\n",
    "    FROM train_features td\n",
    "    LEFT JOIN experience e ON e.tconst == td.tconst\n",
    "    GROUP BY td.tconst\n",
    "''').df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:42.522152Z",
     "start_time": "2024-03-23T13:45:42.499569Z"
    }
   },
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge movie data with the experience level of the writers and directors.\n",
    "# For each film, calculate the number of writers/directors, average experience and total experience\n",
    "\n",
    "test_fixed_title = con.execute('''\n",
    "    SELECT \n",
    "      ANY_VALUE(td.tconst) AS tconst, \n",
    "      ANY_VALUE(td.Year) AS Year, \n",
    "      ANY_VALUE(runtime) AS runtimeMinutes,\n",
    "      ANY_VALUE(ForeignFilm) AS ForeignFilm, \n",
    "      ANY_VALUE(n_words) AS n_words, \n",
    "      ANY_VALUE(votes) AS numVotes,\n",
    "      COUNT(DISTINCT writer) AS n_writers,\n",
    "      AVG(DISTINCT e.writer_experience) AS avgexp_writers,\n",
    "      SUM(DISTINCT e.writer_experience) AS totexp_writers,\n",
    "      COUNT(DISTINCT director) AS n_directors,\n",
    "      AVG(DISTINCT e.director_experience) AS avgexp_directors,\n",
    "      SUM(DISTINCT e.director_experience) AS totexp_directors,\n",
    "      ANY_VALUE(label) AS label\n",
    "    FROM test_features td\n",
    "    LEFT JOIN experience e ON e.tconst == td.tconst\n",
    "    GROUP BY td.tconst\n",
    "''').df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:44.103405Z",
     "start_time": "2024-03-23T13:45:44.080702Z"
    }
   },
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = train_fixed_title.drop(['tconst',  'label'], axis=1).values\n",
    "y_train = train_fixed_title.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:44.719608Z",
     "start_time": "2024-03-23T13:45:44.704803Z"
    }
   },
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = test_fixed_title.drop(['tconst',  'label'], axis=1).values\n",
    "y_test = test_fixed_title.loc[:, 'label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:45.078874Z",
     "start_time": "2024-03-23T13:45:45.067918Z"
    }
   },
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "XGBoost best parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200, 'classifier__subsample': 0.8}\n",
      "XGBoost best score: 0.7141\n",
      "Accuracy: 76.72% 76.71%\n",
      "Confusion Matrix:\n",
      "[[3242  727]\n",
      " [1126 2864]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.82      0.78      3969\n",
      "        True       0.80      0.72      0.76      3990\n",
      "\n",
      "    accuracy                           0.77      7959\n",
      "   macro avg       0.77      0.77      0.77      7959\n",
      "weighted avg       0.77      0.77      0.77      7959\n",
      "\n",
      "0.5013262599469496 0.5013192612137203 0.45118733509234826 0.44937167\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Base model pipelines\n",
    "models = {\n",
    "    'XGBoost': Pipeline(steps=[('preprocessing', preprocessing), ('classifier', xgb.XGBClassifier())])\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 150, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [2, 3, 4, 5],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "best_models = {}\n",
    "for model_name, model_pipeline in models.items():\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(model_pipeline, hyperparameters[model_name], cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"{model_name} best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} best score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Use the best estimator to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Test if adjusting the threshold helps improve the accuracy (knowing dataset is balanced)\n",
    "    y_proba = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    threshold = np.median(y_proba)\n",
    "    y_pred_th = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% {accuracy_score(y_test, y_pred_th) * 100:.2f}%\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # See how balanced the predictions are\n",
    "    print (y_train.mean(), y_test.mean(), y_pred.mean(), threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T13:45:56.347572Z",
     "start_time": "2024-03-23T13:45:50.429660Z"
    }
   },
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
